[
  {
    "text": "Model training is more an art than a science. And in a lot of ways we, as model trainers, think a lot about data quality. It's one of the most important things in model training is like how do you ensure the highest quality data for certain interaction model behavior that you want to create?",
    "speaker": "Karina Nguyen",
    "timestamp": "00:06:11",
    "context": "Lenny asks Karina what people most misunderstand about how AI models are created. Karina explains that model training involves a careful balance of teaching models contradictory information - like telling them they don't have physical bodies while also teaching them to perform functions like setting alarms. She emphasizes that the process requires constant debugging and balancing between making models helpful versus preventing harmful behavior.",
    "vocabulary_highlights": [
      "more an art than a science",
      "model trainers",
      "interaction model behavior"
    ],
    "topics": [
      "Product Management",
      "Strategy & Planning"
    ],
    "difficulty_level": "Advanced",
    "speaker_function": "Engineering",
    "speaker_expertise": [
      "AI/ML Products",
      "User Experience",
      "Data Analytics"
    ],
    "text_ko": "모델 학습(model training)은 과학이라기보다 예술에 더 가깝다고 생각해요. 그리고 여러 면에서 저희 모델 트레이너(model trainers)들은 데이터 품질에 대해 많은 고민을 해요. 모델 학습에서 가장 중요한 것 중 하나는, 원하는 특정 상호작용 모델의 행동(interaction model behavior)을 만들어내기 위해 어떻게 하면 최고 품질의 데이터를 확보하느냐 하는 거예요.",
    "text_zh": "模型训练说白了更像是门艺术，而不是纯科学。我们做模型训练的人，其实很大程度上都在琢磨数据质量这个事儿。在模型训练里头，最关键的就是怎么保证数据质量够高，这样才能训出你想要的那种交互行为。",
    "text_es": "El entrenamiento de modelos es más un arte que una ciencia. Y de muchas maneras nosotros, como entrenadores de modelos (model trainers), pensamos muchísimo en la calidad de los datos. Una de las cosas más importantes en el entrenamiento de modelos es cómo te aseguras de tener datos de la más alta calidad para crear el comportamiento específico que quieres en tu modelo de interacción.",
    "key_sentence": "Model training is more an art than a science, focusing on data quality."
  },
  {
    "text": "When you taught the model, some of the self-knowledge of you actually don't have a physical body to operate in the physical world, the model would get extremely confused. And so the model would get extremely confused about whether it can set an alarm, but it doesn't have a body in the physical world.",
    "speaker": "Karina Nguyen",
    "timestamp": "00:00:42",
    "context": "Lenny asks what people misunderstand about model creation. Karina shares a specific example from training Claude 3 where they discovered conflicts in the training data. The model was taught it has no physical body, but also learned function calls for tasks like setting alarms, creating confusion about what it could actually do. This illustrates the complex debugging process required in model training.",
    "vocabulary_highlights": [
      "self-knowledge",
      "get extremely confused"
    ],
    "topics": [
      "Product Management",
      "Learning & Growth"
    ],
    "difficulty_level": "Intermediate",
    "speaker_function": "Engineering",
    "speaker_expertise": [
      "AI/ML Products",
      "User Experience",
      "Data Analytics"
    ],
    "text_ko": "모델(model)에게 '자기 자신은 실제 세계에서 작동할 물리적 몸이 없다'는 식의 자기 지식(self-knowledge)을 알려주면, 모델이 아주 혼란스러워해요. 그래서 물리적 몸이 없으니 알람을 설정할 수 있는지 같은 문제에서 특히 엄청 헷갈려해요.",
    "text_zh": "你在训练模型的时候，如果告诉它一些关于自己的认知，比如说它其实没有实体身体，不能在现实世界里操作，模型就会特别懵圈。所以模型就会对自己能不能设闹钟这种事情特别纠结，因为它在现实世界里根本就没有身体嘛。",
    "text_es": "Cuando entrenaste el modelo, parte del autoconocimiento de que en realidad no tienes un cuerpo físico para operar en el mundo físico, el modelo se confundía muchísimo. Entonces el modelo se volvía súper confuso sobre si puede poner una alarma, pero no tiene un cuerpo en el mundo físico.",
    "key_sentence": "The model would get extremely confused without self-knowledge of a physical body."
  },
  {
    "text": "I think there are two questions here. We can unpack one at a time. But people say we are hitting the data wall. I think people think more in the terms of pre-trained large models that are trained on the entire internet to predict the next token.",
    "speaker": "Karina Nguyen",
    "timestamp": "00:08:21",
    "context": "Lenny asks about the concern that AI models will stop getting smarter because they're running out of training data. Karina addresses the misconception about the 'data wall' - the idea that models can't improve further because they've already been trained on all available internet data. She argues this view is too narrow and focuses only on pre-training phases.",
    "vocabulary_highlights": [
      "unpack one at a time",
      "hitting the data wall",
      "predict the next token"
    ],
    "topics": [
      "Learning & Growth",
      "Strategy & Planning"
    ],
    "difficulty_level": "Advanced",
    "speaker_function": "Engineering",
    "speaker_expertise": [
      "AI/ML Products",
      "User Experience",
      "Data Analytics"
    ],
    "text_ko": "여기에는 두 가지 질문이 있는 것 같아요. 하나씩 차근차근 풀어볼게요. 그런데 사람들은 우리가 '데이터 장벽(data wall)'에 부딪히고 있다고 말해요. 제 생각엔 사람들이 인터넷 전체를 학습해 다음 토큰을 예측하도록 훈련된, 이른바 사전 학습된 대형 모델(pre-trained large models) 관점으로 더 생각하는 것 같아요.",
    "text_zh": "我觉得这里有两个问题，我们可以一个一个来分析。但是大家都在说我们碰到了数据墙(data wall)。我觉得人们更多想到的是那种在整个互联网上训练的预训练大模型(pre-trained large models)，用来预测下一个token的那种。",
    "text_es": "Creo que aquí hay dos preguntas. Las podemos desglosar una por una. Pero la gente dice que estamos llegando al muro de datos (data wall). Creo que la gente piensa más en términos de modelos grandes preentrenados que se entrenan con todo el internet para predecir el siguiente token.",
    "key_sentence": "We can unpack one at a time; people say we are hitting the data wall."
  },
  {
    "text": "But I think what's happening right now with new paradigm of o1 series is that the scaling in post-training itself is not hitting the wall. And that's because basically we went from raw data sets from pre-trained models to infinite amount of tasks that you can teach the model in the post-training world via reinforcement learning.",
    "speaker": "Karina Nguyen",
    "timestamp": "00:08:47",
    "context": "Lenny asks about synthetic data and whether models will continue getting smarter despite data limitations. Karina explains why she believes the 'data wall' concern is misguided. The o1 series represents a new paradigm where models can be taught infinite tasks during post-training through reinforcement learning, rather than being limited by finite internet data during pre-training.",
    "vocabulary_highlights": [
      "new paradigm",
      "scaling in post-training",
      "hitting the wall",
      "infinite amount of tasks"
    ],
    "topics": [
      "Learning & Growth",
      "Strategy & Planning"
    ],
    "difficulty_level": "Advanced",
    "speaker_function": "Engineering",
    "speaker_expertise": [
      "AI/ML Products",
      "User Experience",
      "Data Analytics"
    ],
    "text_ko": "제 생각에는 지금 o1 시리즈(o1 series)의 새로운 패러다임에서 벌어지는 건, 사후 훈련(post-training) 자체의 스케일링(scaling)이 한계에 닿지 않고 있다는 거예요. 그 이유는 기본적으로 우리가 사전 학습된 모델(pre-trained models)에 쓰던 원시 데이터셋(raw data sets)에서, 사후 훈련 환경에서는 강화학습(reinforcement learning)을 통해 모델에 가르칠 수 있는 태스크(tasks)가 무한히 많아졌기 때문이에요.",
    "text_zh": "但我觉得现在o1系列这个新范式(paradigm)出现后，后训练(post-training)阶段的扩展其实还没碰到瓶颈。主要原因就是，我们从预训练模型(pre-trained models)的原始数据集，转向了后训练阶段通过强化学习(reinforcement learning)能教给模型的无限多任务，这个变化挺大的。",
    "text_es": "Pero creo que lo que está pasando ahora con el nuevo paradigma de la serie o1 es que el escalamiento en el post-entrenamiento no está llegando al límite. Y eso es porque básicamente pasamos de conjuntos de datos en bruto de modelos pre-entrenados a una cantidad infinita de tareas que le puedes enseñar al modelo en el mundo del post-entrenamiento a través del aprendizaje por refuerzo (reinforcement learning).",
    "key_sentence": "The new paradigm shows scaling in post-training is not hitting the wall."
  },
  {
    "text": "Creative thinking and you kind of want to generate a bunch of ideas and filter through them and not just build the best product experience. I think it's actually really, really hard to teach the model how to be aesthetic or really good visual design or how to be extremely creative in the way they write.",
    "speaker": "Karina Nguyen",
    "timestamp": "00:00:20",
    "context": "Lenny asks what skills will be most valuable for product teams as AI advances. Karina emphasizes that creative thinking will become increasingly important - the ability to generate many ideas and filter through them effectively. She explains that current AI models struggle with aesthetic judgment, high-quality visual design, and truly creative writing, making these human skills more valuable.",
    "vocabulary_highlights": [
      "generate a bunch of ideas",
      "filter through them"
    ],
    "topics": [
      "Career Development",
      "Product Management",
      "Learning & Growth"
    ],
    "difficulty_level": "Intermediate",
    "speaker_function": "Engineering",
    "speaker_expertise": [
      "AI/ML Products",
      "User Experience",
      "Data Analytics"
    ],
    "text_ko": "창의적 사고에서는 아이디어를 많이 내서 그중에서 걸러내는 과정이 필요해요. 단순히 최고의 제품 경험(product experience)만 만드는 게 아니죠. 솔직히 모델(model)한테 미적 감각(aesthetic)이나 뛰어난 비주얼 디자인(visual design), 글을 쓸 때 아주 창의적으로 표현하는 법을 가르치는 건 정말, 정말 어렵다고 생각해요.",
    "text_zh": "创意思维就是你得想出一堆点子，然后从中筛选，而不是只盯着做最好的产品体验。我觉得要教模型怎么做出有美感的东西，或者真正好的视觉设计，或者写出特别有创意的文案，这些真的超级难。",
    "text_es": "El pensamiento creativo, ¿sabes?, como que quieres generar un montón de ideas y filtrarlas, no solo crear la mejor experiencia de producto. Creo que en realidad es súper, súper difícil enseñarle al modelo cómo ser estético o tener un diseño visual realmente bueno, o cómo ser extremadamente creativo en la forma de escribir.",
    "key_sentence": "Creative thinking involves generating a bunch of ideas and filtering through them."
  },
  {
    "text": "For Canvas, for example, it came down to three main behaviors. It was how do you trigger Canvas for prompts like, 'Write me a long essay,' when the user intention is mostly iterating over long documents? Or, 'Write me a piece of code,' or when to not trigger Canvas for prompts like, 'Can you tell me more about President...' I don't know, some of the general questions.",
    "speaker": "Karina Nguyen",
    "timestamp": "00:14:28",
    "context": "Lenny asks how Canvas was created using synthetic data training. Canvas is OpenAI's collaborative document editing interface that changes from a chat to a visual editor. Karina explains they identified three core behaviors to teach the model: when to trigger the Canvas interface, how to edit documents, and how to make comments. The first behavior involved teaching the model to distinguish between requests that need document iteration versus simple Q&A.",
    "vocabulary_highlights": [
      "came down to",
      "user intention",
      "iterating over long documents"
    ],
    "topics": [
      "Product Management",
      "Strategy & Planning"
    ],
    "difficulty_level": "Intermediate",
    "speaker_function": "Engineering",
    "speaker_expertise": [
      "AI/ML Products",
      "User Experience",
      "Data Analytics"
    ],
    "text_ko": "예를 들어 캔버스(Canvas)의 경우, 핵심 행동이 크게 세 가지로 정리됐어요. '긴 에세이를 써줘' 같은 프롬프트(prompt)는 사용자가 주로 긴 문서를 반복해서 다듬으려는 의도일 때 캔버스(Canvas)를 언제 불러올지, '코드 한 조각 작성해줘' 같은 경우는 언제 불러올지, 반대로 '대통령에 대해 더 알려줄래?'처럼 일반적인 질문에는 언제 캔버스(Canvas)를 띄우지 말아야 할지에 관한 것이었어요.",
    "text_zh": "拿Canvas来说吧，主要就是三个行为模式。比如什么时候该触发Canvas，像用户说\"帮我写篇长文章\"，但其实用户想要的是在长文档上反复修改；或者\"帮我写段代码\"；还有什么时候不该触发Canvas，比如用户问\"你能跟我说说某某总统...\"这种，就是一些常规问题。",
    "text_es": "Con Canvas, por ejemplo, se reducía a tres comportamientos principales. Era cómo activar Canvas para comandos como \"Escríbeme un ensayo largo\", cuando la intención del usuario es principalmente iterar sobre documentos extensos. O \"Escríbeme un código\", o cuándo no activar Canvas para comandos como \"¿Me puedes contar más sobre el Presidente...?\" No sé, algunas de las preguntas generales.",
    "key_sentence": "Canvas behaviors came down to user intention and iterating over long documents."
  }
]